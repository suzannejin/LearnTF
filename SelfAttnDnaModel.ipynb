{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelTuner import loadDummyData\n",
    "from src.modelTraining.testingModels import SelfAttentionNet, default_params_sattention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mgrapotte/LabWork/LearnTF/src/datasetSimulation/TFFamilyClass.py:71: RuntimeWarning: divide by zero encountered in log2\n",
      "  m = np.log2(ppm.T/bg)\n"
     ]
    }
   ],
   "source": [
    "dataloader = loadDummyData(batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run after this line (dataloader is slow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SelfAttentionNet(params=default_params_sattention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = next(iter((dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "DNA = inputs[0].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_out = model(DNA)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Config setting\n",
    "config = {\n",
    "    'modelconfig' : default_params_sattention,\n",
    "    \"lr\": 1e-2,\n",
    "    \"loss\": \"mse\",\n",
    "    \"optimizer\":\"adam\"}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config, trainloader):\n",
    "    \n",
    "    # initialize model \n",
    "    net = convDNA(config[\"modelconfig\"])\n",
    "\n",
    "    # send model to the right device\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            net = nn.DataParallel(net)\n",
    "    net.to(device)\n",
    "\n",
    "    # setup optimizer and loss according to config\n",
    "    if config[\"optimizer\"] == \"adam\":\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=config[\"lr\"])\n",
    "    elif config[\"optimizer\"] == \"sgd\":\n",
    "        optimizer = torch.optim.SGD(net.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "    if config[\"loss\"] == \"mse\":\n",
    "        criterion = nn.MSELoss()\n",
    "    elif config[\"loss\"] == \"mae\":\n",
    "        criterion = nn.L1Loss()\n",
    "    elif config[\"loss\"] == \"smooth_l1\":\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "\n",
    "    # The `checkpoint_dir` parameter gets passed by Ray Tune when a checkpoint\n",
    "    # should be restored.\n",
    "    if checkpoint_dir:\n",
    "        checkpoint = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "        model_state, optimizer_state = torch.load(checkpoint)\n",
    "        net.load_state_dict(model_state)\n",
    "        optimizer.load_state_dict(optimizer_state)\n",
    "\n",
    "    # train model\n",
    "    for epoch in range(50):\n",
    "        train_loss = 0.0\n",
    "        train_steps = 0\n",
    "        pred = []\n",
    "        true = []\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            dna, prot, labels = data\n",
    "            dna, prot, labels = dna.to(device), prot.to(device), labels.to(device)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(dna)\n",
    "            true += labels.tolist()\n",
    "            pred += outputs.tolist()\n",
    "            loss = criterion(outputs, labels.view(-1,1).float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_steps += 1\n",
    "\n",
    "        with tune.checkpoint_dir(step=epoch) as checkpoint_dir:\n",
    "                    path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
    "                    torch.save(\n",
    "                        (net.state_dict(), optimizer.state_dict()), path)\n",
    "        tune.report(loss=(train_loss / train_steps))# , accuracy=_roc_accuracy(true, pred, 5)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
